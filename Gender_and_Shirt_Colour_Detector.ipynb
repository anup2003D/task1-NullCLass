{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf1cae6a-9fe4-48d9-a390-5f1fe293b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, MaxPool2D, Conv2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Activation,Add\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam,Adagrad,Adadelta, Adamax, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dropout, Activation, MaxPooling2D, Flatten, Dense\n",
    "from keras.layers import Conv2D, Dropout, Activation, Input, MaxPooling2D, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "586d221a-de15-43c5-af74-212ca6a7ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=(\"C:/Users/Anup0/Data Science/Gender and Shirt Colour Detector/tshirt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b0ff63f-272b-44f7-8a4c-e7e764c26ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the T-Shirts dataset\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg') or file.endswith('.png'):\n",
    "            image_path = os.path.join(root, file)\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.resize(image, (224, 224))\n",
    "            data.append(image)\n",
    "            label = os.path.basename(root)\n",
    "            labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20c2da50-59d5-4b24-87c8-37944446e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e3b347-51da-473c-8aaa-96ec42dc25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "labels = to_categorical(labels, num_classes=len(np.unique(labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27e901e2-2459-40cd-a8c5-bdb115a8c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize image data\n",
    "data = data.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2503b07d-9093-4101-abda-e16850e59f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model = load_model(\"C:/Users/Anup0/Data Science/Age and Gender Detector/Age_Sex_Detection.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd8f2a1f-4ef9-4472-87ee-0d716e03a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the model\n",
    "model.layers.pop()\n",
    "x = model.layers[-1].output\n",
    "shirt_color_output = Dense(len(np.unique(labels)), activation='softmax', name='shirt_color_output')(x)\n",
    "\n",
    "new_model = Model(inputs=model.input, outputs=shirt_color_output)\n",
    "new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f12c570-7467-4d42-ac0e-7a8160099544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anup0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Use sigmoid for binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f375962c-5296-49b3-a711-974e54e670d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca4ed70f-86e7-4331-bedd-2dcbad1921ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0550 - val_accuracy: 1.0000 - val_loss: 1.4408e-18\n",
      "Epoch 2/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 1.9023e-21 - val_accuracy: 1.0000 - val_loss: 1.0673e-18\n",
      "Epoch 3/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 7.1127e-22 - val_accuracy: 1.0000 - val_loss: 1.0651e-18\n",
      "Epoch 4/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 1.6791e-21 - val_accuracy: 1.0000 - val_loss: 1.0651e-18\n",
      "Epoch 5/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 2.1573e-21 - val_accuracy: 1.0000 - val_loss: 1.0651e-18\n",
      "Epoch 6/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 1.4238e-21 - val_accuracy: 1.0000 - val_loss: 1.0651e-18\n",
      "Epoch 7/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 8.1788e-22 - val_accuracy: 1.0000 - val_loss: 1.0651e-18\n",
      "Epoch 8/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 6.2904e-22 - val_accuracy: 1.0000 - val_loss: 1.0651e-18\n",
      "Epoch 9/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 1.6511e-21 - val_accuracy: 1.0000 - val_loss: 1.0651e-18\n",
      "Epoch 10/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 8.9501e-22 - val_accuracy: 1.0000 - val_loss: 1.0651e-18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2922ce8d460>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Assume data is a numpy array of shape (num_samples, 224, 224, 3)\n",
    "data_resized = np.array([cv2.resize(image, (48, 48)) for image in data])\n",
    "\n",
    "# Now you can train the model\n",
    "new_model.fit(data_resized, labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c33c057f-9929-4286-a5a9-fd4efcc8971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated model\n",
    "new_model.save('Gender_Shirt_Color_Detection.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40b2e35a-eab7-4662-a666-6fb5868fca6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meeting data shape: (15, 224, 224, 3)\n",
      "Meeting labels shape: (15, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Path to your custom dataset of meeting images\n",
    "custom_dataset_path = r\"C:\\Users\\Anup0\\Data Science\\Gender and Shirt Colour Detector\\Meeting\"\n",
    "\n",
    "# Initialize lists to hold image data and labels\n",
    "meeting_data = []\n",
    "meeting_labels = []\n",
    "\n",
    "# Traverse the dataset directory and collect image data and labels\n",
    "for root, dirs, files in os.walk(custom_dataset_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg') or file.endswith('.png') or file.endswith('.webp'):\n",
    "            image_path = os.path.join(root, file)\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is not None:\n",
    "                image = cv2.resize(image, (224, 224))\n",
    "                meeting_data.append(image)\n",
    "                # Manually labeled shirt color (example)\n",
    "                label = 'white'  # Replace with actual logic to assign the correct label\n",
    "                meeting_labels.append(label)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "meeting_data = np.array(meeting_data)\n",
    "meeting_labels = np.array(meeting_labels)\n",
    "\n",
    "# Encode labels and convert to categorical format\n",
    "label_encoder = LabelEncoder()\n",
    "meeting_labels = label_encoder.fit_transform(meeting_labels)\n",
    "meeting_labels = to_categorical(meeting_labels, num_classes=len(np.unique(meeting_labels)))\n",
    "\n",
    "# Normalize image data\n",
    "meeting_data = meeting_data.astype('float32') / 255.0\n",
    "\n",
    "# Print the shapes of the arrays to verify\n",
    "print(f\"Meeting data shape: {meeting_data.shape}\")\n",
    "print(f\"Meeting labels shape: {meeting_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f9e35f2-4715-4a5d-91c7-eb50c110da8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meeting data shape: (15, 224, 224, 3)\n",
      "Meeting labels shape: (15, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 1.0000 - loss: 1.8448e-23\n",
      "Validation Loss: 1.844843208040885e-23\n",
      "Validation Accuracy: 1.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Predicted: white, Actual: white\n",
      "Predicted: white, Actual: white\n",
      "Predicted: white, Actual: white\n",
      "Predicted: white, Actual: white\n",
      "Predicted: white, Actual: white\n",
      "Predicted: white, Actual: white\n",
      "Predicted: white, Actual: white\n",
      "Predicted: white, Actual: white\n",
      "Predicted: white, Actual: white\n",
      "Predicted: white, Actual: white\n",
      "Predicted: white, Actual: white\n",
      "Predicted: white, Actual: white\n",
      "Predicted: white, Actual: white\n",
      "Predicted: white, Actual: white\n",
      "Predicted: white, Actual: white\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Path to your custom dataset of meeting images\n",
    "custom_dataset_path = r\"C:\\Users\\Anup0\\Data Science\\Gender and Shirt Colour Detector\\Meeting\"\n",
    "\n",
    "# Initialize lists to hold image data and labels\n",
    "meeting_data = []\n",
    "meeting_labels = []\n",
    "\n",
    "# Traverse the dataset directory and collect image data and labels\n",
    "for root, dirs, files in os.walk(custom_dataset_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg') or file.endswith('.png') or file.endswith('.webp'):\n",
    "            image_path = os.path.join(root, file)\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is not None:\n",
    "                image = cv2.resize(image, (224, 224))\n",
    "                meeting_data.append(image)\n",
    "                # Manually labeled shirt color (example)\n",
    "                label = 'white'  # Replace with actual logic to assign the correct label\n",
    "                meeting_labels.append(label)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "meeting_data = np.array(meeting_data)\n",
    "meeting_labels = np.array(meeting_labels)\n",
    "\n",
    "# Encode labels and convert to categorical format\n",
    "label_encoder = LabelEncoder()\n",
    "meeting_labels = label_encoder.fit_transform(meeting_labels)\n",
    "meeting_labels = to_categorical(meeting_labels, num_classes=len(np.unique(meeting_labels)))\n",
    "\n",
    "# Normalize image data\n",
    "meeting_data = meeting_data.astype('float32') / 255.0\n",
    "\n",
    "# Print the shapes of the arrays to verify\n",
    "print(f\"Meeting data shape: {meeting_data.shape}\")\n",
    "print(f\"Meeting labels shape: {meeting_labels.shape}\")\n",
    "\n",
    "# Flatten the data if the model requires a 2D input\n",
    "# Check the model input shape\n",
    "model = load_model(\"C:/Users/Anup0/Data Science/Gender and Shirt Colour Detector/Gender_Shirt_Color_Detection.keras\")\n",
    "input_shape = model.input_shape\n",
    "\n",
    "if len(input_shape) == 2:  # Model expects 2D input\n",
    "    meeting_data = meeting_data.reshape(meeting_data.shape[0], -1)\n",
    "elif len(input_shape) == 4 and input_shape[1:] != (224, 224, 3):\n",
    "    # If model expects a different shape, resize accordingly\n",
    "    meeting_data = np.array([cv2.resize(img, (input_shape[1], input_shape[2])) for img in meeting_data])\n",
    "\n",
    "# Evaluate the model on the custom meeting dataset\n",
    "loss, accuracy = model.evaluate(meeting_data, meeting_labels)\n",
    "print(f'Validation Loss: {loss}')\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "\n",
    "# Predict shirt colors on the custom meeting dataset\n",
    "predictions = model.predict(meeting_data)\n",
    "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "for i in range(len(meeting_data)):\n",
    "    print(f'Predicted: {predicted_labels[i]}, Actual: {label_encoder.inverse_transform([np.argmax(meeting_labels[i])])[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab019629-394c-48c7-9629-22531aeac25e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
